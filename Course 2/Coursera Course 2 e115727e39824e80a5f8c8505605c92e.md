# Coursera Course 2

# Week 1

## Train/Dev/Test Set

![Screenshot from 2024-07-19 10-22-09.png](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Screenshot_from_2024-07-19_10-22-09.png)

![Screenshot from 2024-07-19 10-22-23.png](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Screenshot_from_2024-07-19_10-22-23.png)

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled.png)

We split data as Training - on which model is trained; Development - on which few best models are observed; Test - to get unbiased accuarcy of the best model. Presence of test set is optional. 

## Bias/Variance

![Screenshot from 2024-07-19 10-45-48.png](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Screenshot_from_2024-07-19_10-45-48.png)

![Screenshot from 2024-07-19 10-46-02.png](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Screenshot_from_2024-07-19_10-46-02.png)

![Screenshot from 2024-07-19 10-46-16.png](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Screenshot_from_2024-07-19_10-46-16.png)

We assume here a small valjue of optimal (Baye’s Error)

## Basic Receipe for Machine Learning

![Screenshot from 2024-07-19 10-57-34.png](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Screenshot_from_2024-07-19_10-57-34.png)

These steps can be fllowed for reducing values of bias and variance. Regularization may increase the bias a bit but not too much. Training bigger network reduces bias almost independent of the value of variance and vice versa, for using a larger dev set.

## Regularization

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%201.png)

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%202.png)

The denominator can be any other term than 2 as well; its just a scaling constant.. Sparse w means that it contains a lot of zeros; it occurs in L1 regularization. It can be helpful as it will require less memory but it isn’t to useful; L2 regularization is mostly used. For neural networks we use Frobenius Norm. The L2 regularization that we use in DL is termed as ‘Weight Decay’.

## Why regularization reduces overfitting?

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%203.png)

The regularization doesn;t set all the weights cloe to zero but it simply reduces the impart of the alrge number of hidden units while using all of them, which, kind of turns the NN to a simpler Logistic Regression Model, like thugh it has smaller number of hidden units and reduce variance.

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%204.png)

Also, in L2 regularization, due to small W, with high lambda, we get linear part f actiavtion function which tends to make the effect of several hidden units lesser.

## Dropout Regularization

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%205.png)

![Not to use dropout reg. in Testing](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%206.png)

Not to use dropout reg. in Testing

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%207.png)

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%208.png)

## Some more Regularization Techniques

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%209.png)

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2010.png)

Early stopping disobeys Orthogonalisation; a better way to do so would be L2 Regularisation but it can be computationally expensive due to various values of lambda.

## Normalising Inputs

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2011.png)

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2012.png)

Normalisation is to be done with same parameters on both training and test sets. It helps to bring different parameters on different scales, on similar scales, which speeds up gradient descent.

## Vanishing/Exploding Gradients

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2013.png)

# Week 3

# Tuning Process

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2014.png)

There are several types of hyperparameters used in deep nets as shown above. We can organise them priority wise.

One way to find the best values, for instance out of 2 hyperparameters, is to plot some random points in a 2-D space, where each axis denotes each of those 2 hyperparameters. Choosing uniform points leads to inaccuracy due to redundancy of any of the hyperparameters. Hence we choose random points. Same idea can be extended for multiple hyperparameters. Also, after we find a best point in this grid, optionally, we can choose to repeat this process for a small region around it. This is known as going from coarse to fine.

# Scale of Hyperparameter tuning

For scaling integral hyperparameters we can scale randomly e.g. no. of hidden units/layers. But for hyperparameters like learning rate, which can lie between 0.0001 and 0.1, most of the points would occur between 0.001 and 0.1; so in such case we use logarithmic scales.

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2015.png)

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2016.png)

Even if scaling goes less accurately, still it can be improved by coarse to fine method.

# Panva vs Caviar Approach

The hyperparameters must be periodically checked for their effectiveness.

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2017.png)

Panda approach -

- When less computational resources
- Vary parameters by checking past progress
- If found poor progress can change model and retrain, but only 1 model at a time.

Caviar Approach -

- When large number of computational resources available, train a model with a set of hyperparameters on each
- Map their overall progress (of data error/cost function w.r.t. time) and choose best set of hyperparametrs.
- This approach is preferred

# Normalising Activations in a network

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2018.png)

We tend to have zero mean and unit standard variance for input layer in logistic regression for better optimization of W and b. Likewise, we implement it on the value z of each layer of DNN (this is also done for activations rather than z, but more commonly for z). Known as ‘Batch Norm’.We can apply the computations for zero mean and unit standard variance for z[l] and then modify the equation using the hyperparameters gamma and beta. This is because we may necessarily want zero mean and unit standard variance due to which activation of that layer, for e.g. sigmoid function, will lie in the central linear range.

![Untitled](Coursera%20Course%202%20e115727e39824e80a5f8c8505605c92e/Untitled%2019.png)