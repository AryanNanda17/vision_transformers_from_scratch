# Course 4 : CNNs

WEEK 1 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%201.png)

the below image is of more pixels like 1000 * 1000 * 3
so, no. of input parameters = 3 million so W is of shape  [1000, 3 million] so total we have 3 billion parameters

For computer vision applications, you don't want to be stuck using only tiny little images. You want to use large images. To do that, you need to better implement the **convolution operation**, which is one of the fundamental building blocks of **convolutional neural networks**.

**Edge Detection Example**

Convolution Operator

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%202.png)

Convolving it with the vertical edge detection filter results in detecting the vertical edge down the middle of the image

A `3 by 3` filter or `3 by 3` matrix may look like below, and this is called a vertical edge detector or a vertical edge detection filter. In this matrix, pixels are relatively bright on the left part and relatively dark on the right part.

**1, 0, -1
1, 0, -1
1, 0, -1**

The *convolution operation* gives you a convenient way to specify how to find these **vertical edges** in an image.

**More Edge Detection**

-30 show that there is a light to dark transition

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%203.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%204.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%205.png)

Different filters allow you to find vertical and horizontal edges. The following filter is called a **Sobel filter** the advantage of which is it puts a little bit more weight to the central row, the central pixel, and this makes it maybe a little bit more robust.

> 
> 
> 
> 1, 0, -1
> 2, 0, -2
> 1, 0, -1
> 

Here is another filter called **Scharr filter**:

> 3, 0, -3
10, 0, -10
3, 0, -3
> 

> w1, w2, w3
w4, w5, w6
w7, w8, w9
> 

By just letting all of these numbers be parameters and learning them automatically from data, we find that neural networks can actually learn low level features, can learn features such as edges, even more robustly than computer vision researchers are generally able to code up these things by hand.

**Padding:**

Problems with convolution: 
1)  Every time you apply a convolutional operator the image shrinks. Like a 6 * 6 image shrinks to 4 * 4 if the filter is 3 * 3. so after a lot of layers, we end up with a very small image

2) The edge pixel is used only once for the convolution, while the middle  ones are used a lot more times. Thus, a lot of information from the edges of the image is thrown away.

**SOLUTION : padding**

6 * 6 padded to 8 * 8 (we can pad with even more pixels)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%206.png)

so after convolving we end up with 6 * 6

*Notations*:

- image size: `n x n`
- convolution size: `f x f`
- padding size: `p`

*Output size after convolution*:

- without padding: `(n-f+1) x (n-f+1)`
- with padding: `(n+2p-f+1) x (n+2p-f+1)`

*Convention*:

- Valid convolutions: no padding
- Same convolutions: output size is the same as the input size
- `f` is usually odd

[https://www.notion.so](https://www.notion.so)

**Strided Convolutions**

round off to floor

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%207.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%208.png)

This is the dimensions of the current output image , while n is the dimensions of the previous image n[l -1]

 **Convolutions Over Volume**

for multilayers(many channels), the filter will also have channels 
i.e For a RGB image, the filter itself has three layers corresponding to the red, green, and blue channels.

If you want to detect edges in the first channel(say red) then keep the first filter as -
 `1 0 -1
 1 0 -1
 1 0 -1`
and all the other 2 filters can be zero

How output of the 4 * 4 image came?

the 27 parameters (3 * 3 * 3) of the filter get multiplied by the image one by one
the 27 values add up and give one value of 4 * 4 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%209.png)

**Multiple Filters :**

Suppose the first filter is used for vertical edge detection and the second for horizontal edge detection. The o**utput** then contains image with **2 layers**

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2010.png)

### **One Layer of a Convolutional Network(IMP)**

what we applied to each neuron, here we will apply to patches

The input image is X, the filters are weights, and the output is A

Here, 2 filters indicate 2 input features

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2011.png)

Note that for any layer, since the filter overlaps and moves over the image, so the filter in the l^th layer must have channels same as the input layer in the (l - 1)^th layer.  And the channels of the input layer in the (l - 1t th layer depend on the number of filters in (l - 1)th layer. eg. here The output is 4 * 4 * 2  i.e channels = 2 = number of filters in that layer

$n_h = height \ of \ channel ,  \\ n_w = width \ of \ channel$

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2012.png)

ans = each filter has 27 + 1 = 28 parameters , so total 10 filters will have 280 parameters 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2013.png)

**Weights parameters = $f^{[l]} * f^{[l]} * n_c^{[l-1]} * n_c^{[l]}$** 

**explanation: The image in the $l^{th}$ layer will have as many as channels as there as filters in the ${(l-1})^{th}$  layer. so total parameters in one image = $f^{[l]} * f^{[l]} * n_c^{[l-1]}$** 

**so if there are ten filters then no. of parameters = $f^{[l]} * f^{[l]} * n_c^{[l-1]} * n_c^{[l]}$** 

**Remember = The channels of the output layer depend on the numbe of filters present in that layer**

**so each filter = $f^{[l]}$*** $f^{[l]} * n_c^{[l-1]}$

**p=0 means valid convolutions**

37 comes from [n + 2p - f / s] + 1 formula and 10 channels comes from 10 filters we have used. Similarly 17 and 7 come

10, 20 , 40 in the activations are coming from the number of filters used 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2014.png)

In the last step, we take all the vectors and put them into one long vector and apply logistic regression / softmax on it

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2015.png)

**Pooling Layers**

1) Max pooling

Max pooling is a downsampling technique used in Convolutional Neural Networks (CNNs) to reduce the spatial dimensions (width and height) of the input data while **retaining the most important features**

usually we do not use any padding 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2016.png)

- Note 1 = number of channels in input = number of channels in output
- Note 2 = There are no parameters to learn

2) average pooling( not used oftenly ) -

 used to collapse 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2017.png)

number of channels = num of filters in that layer

conv 1 and pool 1 together form the layer 1(since a layer is something that has weights and pool does not have these) 
note that pool has same channels as that of conv

Lets use 6 filters in layer 1, 10 in layer 2

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2018.png)

FC3 = fully connected layer 3

as we go deeper, the height and width decreases and the number of channels increases

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/1fc11506-cd9d-47ec-a932-7025f08ae1a6.png)

POOL has no learnable parameters

To get the total number of parameters (weights and biases) in the network:

1. **CONV1**:
    - Parameters: (5×5×3)×6+6=456
        
        (5×5×3)×6+6=456
        
    - Explanation: Each of the 6 filters has 5×5×3=75 weights (since the input has 3 channels), plus 1 bias per filter. So, 75×6+6=456.
2. **POOL1**:
    - No parameters (pooling layers do not have parameters).
3. **CONV2**:
    - Parameters: (5×5×6)×16+16=2,416
    - Explanation: Each of the 16 filters has 5×5×6=150 weights (since the input has 6 channels), plus 1 bias per filter. So, 150×16+16=2,416.
        
        
4. **POOL2**:
    - No parameters (pooling layers do not have parameters).
5. **FC3**: (fully convolution layer just like we saw in course 1 and 2)
    - Parameters: 400×120+120=48,120
6. **FC4**:
    - Parameters: 120×84+84=10,164
7. **Output Layer**:
    - Parameters: 84×10+10=850
    
    ![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2019.png)
    

number of channels in current layer = number of filters

**Why Convolutions?**

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2020.png)

Through convolutions(filters) we have only 456 params but w/o it it is 3072 * 4704 = 15 million params

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2021.png)

sparse connection - means that the 1 pixel of the output is connected to only 9 of the pixel in the input (since filter = 3 * 3) 

Through these two mechanisms, a neural network has a lot fewer parameters which allows it to be trained with smaller training cells and is less prone to be overfitting.

- Convolutional structure helps the neural network encode the fact that an image shifted a few pixels should result in pretty similar features and should probably be assigned the same output label.
- And the fact that you are applying the same filter in all the positions of the image, both in the early layers and in the late layers that helps a neural network automatically learn to be more robust or to better capture the desirable property of translation invariance.

image → convolutional layers → fully connected layers → output layer

W, b ⇒ randomly initialised

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2022.png)

quiz :

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2023.png)