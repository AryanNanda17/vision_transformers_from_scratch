# Deep Learning

# Course 1: Neural Networks and Deep Learning

This is the most simple neural network:

Here we try to fit a line through the given data, Here we only have one input i.e. size of house and price as the output.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled.png)

Consider you have multiple structures like this forms a bigger neural network

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%201.png)

This is a form of **Supervised Learning.**

### Supervised Learning:

Supervised learning is a machine learning technique that uses labeled data to train algorithms to predict outcomes and recognize patterns.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%202.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%203.png)

### Why is Deep Learning taking off ?

Driving factors - Scale and Amount of Data, also computation(GPU, CPU) and algorithms(sigmoid, ReLU)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%204.png)

## Binary Classification

Logistic Regression is an algorithm to perform Binary Classification,

e.g. An image is of a cat or not.

For example, we can arrange all the pixel values of an image into a column matrix.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%205.png)

these are the notations that will be used for Logistic Regression.

### Logistic Regression

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%206.png)

Keeping b separate is better than having it in the same matrix as w as shown in top right section.

### Logistic Regression Cost Function

Loss(error) function finds difference between predicted output and output by the network, here squared error is not used because then the optimisation problem becomes non-convex

We use a different loss function, 

loss function is for a single training example.

Cost function is the average of the loss functions of the entire training data set.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%207.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%208.png)

## Gradient Descent

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%209.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2010.png)

### Computation Graph

A computation graph is a conceptual and visual representation of the operations and data flow involved in a computational process,organized as a directed acyclic graph (DAG) where nodes represent operations or variables, and edges represent the flow of data (tensors).

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2011.png)

We apply chain rule to find derivatives through this graph.

### Logistic Regression gradient

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2012.png)

Step 1: We first initialise the parameters at random or zero

Step 2: We iterate through the m training examples and calculate the sum of gradient of the loss with respect to each parameter.

Step 3: We subtract this gradient of each parameter multiple by alpha i.e Learning rate.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2013.png)

We do Vectorization to group this parameters and reduce computation time in the code.

---

## Vectorization

It improves computation time by a lot using NumPy as for loops take more time.

Here in this python snippet we can see that using numpy arrays and its function uses less time:

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2014.png)

We should avoid using explicit for loops if something can be achieved using matrix operations

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2015.png)

even if b is just a number python automatically converts the b into a 1xm matrix.(Broadcasting)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2016.png)

**Derivation of these derivatives:**

https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60

## Broadcasting

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2017.png)

---

It is a feature in numpy where if a matrix is compatible with another matrix for element-wise operations but just lacks a dimension it will broadcast/duplicate the existing elements as per the case.

Rules for broadcasting:

- Rule 1: If the arrays have different dimensions, prepend ones to the shape of the smaller array until they have the same number of dimensions.
- Rule 2: The arrays are compatible for broadcasting if their dimensions satisfy one of the following conditions:
    - The dimensions are equal.
    - One of the dimensions is 1.

## Neural Networks

the input layer is not considered as a part of the NN, so we called it the zero-th layer and the layer after the input is called the first layer.

**Notation:**

**In superscript:**

 **[ ] → Layer index, ( ) → index of the training example** 

**In subscript:**

**Index of the neuron**

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2018.png)

**W.T** → Transpose

---

## Activation functions

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2019.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2020.png)

**Sigmoid -** Mostly compatible for binary classification problems, not used because of the vanishing gradient problem having slow learning time and network gets stuck and cannot improve further

**tanh -** Better than sigmoid, used in hidden layers, avoids vanishing gradient problems

**ReLU -** has a simpler gradient ( 0 or 1 ) less susceptible to vanishing gradient problems

### Why do we need a non-linear activation function?

If a NN has activation which are linear only then the model is applying only linear transformations and will not be able to catch up complex patterns in the dataset, a non linear activation function makes that possible. If there is a linear activation functions the hidden layers does not make the model any better.

Linear activation functions can only be used in **linear regression** problems.

---

## Gradient Descent

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2021.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2022.png)

---

## Random Initialization

If all weights start at zero, then all neurons in a hidden layer will receive the same input (assuming the same activation function for all neurons). This creates symmetry in the network. Then after applying back propagation, the gradients also become insignificant and makes the model inefficient or useless.

For sigmoid and tanh, the values for W should be small and random, because if the input are larger the gradient will be zero and not much meaningful changes will be made to the weights. 

---

## Deep L-layer Neural Network

Logistic regression is a very shallow neural network, Now we will apply a deep neural network with more than one hidden layers

**Notation:** same as previous.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2023.png)

This is how we get the shapes of all the matrices used.

But this is for one training example, We use Uppercase letters for all the trainng examples.

the order of X is (n[0],m), so we get Z also for all training examples, b gets broadcasted to (n[L] , m)

**Some intuition behind what the multiple layers are doing:**                                                                  

Initial layers learn to detect simpler details and the layer later combine all of these to get the final output.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2024.png)

**Why deep networks are better?**

Let’s say we want to perform the XOR operation on n data points then for a shallow networks with one hidden layer would need 2 ^ (n-1) neurons for this operations but a Deep network can do this in as less as log(n) units.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2025.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2026.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2027.png)

Here the forward function calculates Z then applies the specified activation functions and calculates the activations and also stores the Z and A in cache for the backward propagation step.

## Parameters and Hyperparameters

**Parameters: W, b**

**Hyperparameters:** 

1. Learning rate
2. iterations
3. hidden layers
4. hidden units

Some we will learn later: Momentum, minibatch size, etc.

---

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2028.png)