# Deep Learning

# Course 1: Neural Networks and Deep Learning

This is the most simple neural network:

Here we try to fit a line through the given data, Here we only have one input i.e. size of house and price as the output.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled.png)

Consider you have multiple structures like this forms a bigger neural network

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%201.png)

This is a form of **Supervised Learning.**

### Supervised Learning:

Supervised learning isÂ a machine learning technique that uses labeled data to train algorithms to predict outcomes and recognize patterns.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%202.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%203.png)

### Why is Deep Learning taking off ?

Driving factors - Scale and Amount of Data, also computation(GPU, CPU) and algorithms(sigmoid, ReLU)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%204.png)

## Binary Classification

Logistic Regression is an algorithm to perform Binary Classification,

e.g. An image is of a cat or not.

For example, we can arrange all the pixel values of an image into a column matrix.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%205.png)

these are the notations that will be used for Logistic Regression.

### Logistic Regression

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%206.png)

Keeping b separate is better than having it in the same matrix as w as shown in top right section.

### Logistic Regression Cost Function

Loss(error) function finds difference between predicted output and output by the network, here squared error is not used because then the optimisation problem becomes non-convex

We use a different loss function, 

loss function is for a single training example.

Cost function is the average of the loss functions of the entire training data set.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%207.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%208.png)

## Gradient Descent

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%209.png)

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2010.png)

### Computation Graph

A computation graph is a conceptual and visual representation of the operations and data flow involved in a computational process,organized as a directed acyclic graph (DAG) where nodes represent operations or variables, and edges represent the flow of data (tensors).

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2011.png)

We apply chain rule to find derivatives through this graph.

### Logistic regression gradient

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2012.png)

Step 1: We first initialise the parameters at random or zero

Step 2: We iterate through the m training examples and calculate the sum of gradient of the loss with respect to each parameter.

Step 3: We subtract this gradient of each parameter multiple by alpha i.e Learning rate.

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2013.png)

We do Vectorization to group this parameters and reduce computation time in the code.

### Vectorization

It improves computation time by a lot using NumPy as for loops take more time.

Here in this python snippet we can see that using numpy arrays and its function uses less time:

![Untitled](Deep%20Learning%2074836342be474166bcc6a5d270e7a126/Untitled%2014.png)

We should avoid using explicit for loops if something can be achieved using matrix operations