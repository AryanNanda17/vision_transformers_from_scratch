# Course 4 : CNNs

WEEK 1 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%201.png)

the below image is of more pixels like 1000 * 1000 * 3
so, no. of input parameters = 3 million so W is of shape  [1000, 3 million] so total we have 3 billion parameters

For computer vision applications, you don't want to be stuck using only tiny little images. You want to use large images. To do that, you need to better implement the **convolution operation**, which is one of the fundamental building blocks of **convolutional neural networks**.

**Edge Detection Example**

Convolution Operator

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%202.png)

Convolving it with the vertical edge detection filter results in detecting the vertical edge down the middle of the image

A `3 by 3` filter or `3 by 3` matrix may look like below, and this is called a vertical edge detector or a vertical edge detection filter. In this matrix, pixels are relatively bright on the left part and relatively dark on the right part.

**1, 0, -1
1, 0, -1
1, 0, -1**

The *convolution operation* gives you a convenient way to specify how to find these **vertical edges** in an image.

**More Edge Detection**

-30 show that there is a light to dark transition

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%203.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%204.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%205.png)

Different filters allow you to find vertical and horizontal edges. The following filter is called a **Sobel filter** the advantage of which is it puts a little bit more weight to the central row, the central pixel, and this makes it maybe a little bit more robust.

> 
> 
> 
> 1, 0, -1
> 2, 0, -2
> 1, 0, -1
> 

Here is another filter called **Scharr filter**:

> 3, 0, -3
10, 0, -10
3, 0, -3
> 

> w1, w2, w3
w4, w5, w6
w7, w8, w9
> 

By just letting all of these numbers be parameters and learning them automatically from data, we find that neural networks can actually learn low level features, can learn features such as edges, even more robustly than computer vision researchers are generally able to code up these things by hand.

**Padding:**

Problems with convolution: 
1)  Every time you apply a convolutional operator the image shrinks. Like a 6 * 6 image shrinks to 4 * 4 if the filter is 3 * 3. so after a lot of layers, we end up with a very small image

2) The edge pixel is used only once for the convolution, while the middle  ones are used a lot more times. Thus, a lot of information from the edges of the image is thrown away.

**SOLUTION : padding**

6 * 6 padded to 8 * 8 (we can pad with even more pixels)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%206.png)

so after convolving we end up with 6 * 6

*Notations*:

- image size: `n x n`
- convolution size: `f x f`
- padding size: `p`

*Output size after convolution*:

- without padding: `(n-f+1) x (n-f+1)`
- with padding: `(n+2p-f+1) x (n+2p-f+1)`

*Convention*:

- Valid convolutions: no padding
- Same convolutions: output size is the same as the input size
- `f` is usually odd

[https://www.notion.so](https://www.notion.so)

**Strided Convolutions**

round off to floor

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%207.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%208.png)

This is the dimensions of the current output image , while n is the dimensions of the previous image n[l -1]

 **Convolutions Over Volume**

for multilayers(many channels), the filter will also have channels 
i.e For a RGB image, the filter itself has three layers corresponding to the red, green, and blue channels.

If you want to detect edges in the first channel(say red) then keep the first filter as -
 `1 0 -1
 1 0 -1
 1 0 -1`
and all the other 2 filters can be zero

How output of the 4 * 4 image came?

the 27 parameters (3 * 3 * 3) of the filter get multiplied by the image one by one
the 27 values add up and give one value of 4 * 4 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%209.png)

**Multiple Filters :**

Suppose the first filter is used for vertical edge detection and the second for horizontal edge detection. The o**utput** then contains image with **2 layers**

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2010.png)

### **One Layer of a Convolutional Network(IMP)**

what we applied to each neuron, here we will apply to patches

The input image is X, the filters are weights, and the output is A

Here, 2 filters indicate 2 input features

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2011.png)

Note that for any layer, since the filter overlaps and moves over the image, so the filter in the l^th layer must have channels same as the input layer in the (l - 1)^th layer.  And the channels of the input layer in the (l - 1t th layer depend on the number of filters in (l - 1)th layer. eg. here The output is 4 * 4 * 2  i.e channels = 2 = number of filters in that layer

$n_h = height \ of \ channel ,  \\ n_w = width \ of \ channel$

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2012.png)

ans = each filter has 27 + 1 = 28 parameters , so total 10 filters will have 280 parameters 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2013.png)

**Weights parameters = $f^{[l]} * f^{[l]} * n_c^{[l-1]} * n_c^{[l]}$** 

**explanation: The image in the $l^{th}$ layer will have as many as channels as there as filters in the ${(l-1})^{th}$  layer. so total parameters in one image = $f^{[l]} * f^{[l]} * n_c^{[l-1]}$** 

**so if there are ten filters then no. of parameters = $f^{[l]} * f^{[l]} * n_c^{[l-1]} * n_c^{[l]}$** 

**Remember = The channels of the output layer depend on the numbe of filters present in that layer**

**so each filter = $f^{[l]}$*** $f^{[l]} * n_c^{[l-1]}$

**p=0 means valid convolutions**

37 comes from [n + 2p - f / s] + 1 formula and 10 channels comes from 10 filters we have used. Similarly 17 and 7 come

10, 20 , 40 in the activations are coming from the number of filters used 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2014.png)

In the last step, we take all the vectors and put them into one long vector and apply logistic regression / softmax on it

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2015.png)

**Pooling Layers**

1) Max pooling

Max pooling is a downsampling technique used in Convolutional Neural Networks (CNNs) to reduce the spatial dimensions (width and height) of the input data while **retaining the most important features**

usually we do not use any padding 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2016.png)

- Note 1 = number of channels in input = number of channels in output
- Note 2 = There are no parameters to learn

2) average pooling( not used oftenly ) -

 used to collapse 

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2017.png)

number of channels = num of filters in that layer

conv 1 and pool 1 together form the layer 1(since a layer is something that has weights and pool does not have these) 
note that pool has same channels as that of conv

Lets use 6 filters in layer 1, 10 in layer 2

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2018.png)

FC3 = fully connected layer 3

as we go deeper, the height and width decreases and the number of channels increases

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/1fc11506-cd9d-47ec-a932-7025f08ae1a6.png)

POOL has no learnable parameters

To get the total number of parameters (weights and biases) in the network:

1. **CONV1**:
    - Parameters: (5×5×3)×6+6=456
        
        (5×5×3)×6+6=456
        
    - Explanation: Each of the 6 filters has 5×5×3=75 weights (since the input has 3 channels), plus 1 bias per filter. So, 75×6+6=456.
2. **POOL1**:
    - No parameters (pooling layers do not have parameters).
3. **CONV2**:
    - Parameters: (5×5×6)×16+16=2,416
    - Explanation: Each of the 16 filters has 5×5×6=150 weights (since the input has 6 channels), plus 1 bias per filter. So, 150×16+16=2,416.
        
        
4. **POOL2**:
    - No parameters (pooling layers do not have parameters).
5. **FC3**: (fully convolution layer just like we saw in course 1 and 2)
    - Parameters: 400×120+120=48,120
6. **FC4**:
    - Parameters: 120×84+84=10,164
7. **Output Layer**:
    - Parameters: 84×10+10=850
    
    ![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2019.png)
    

number of channels in current layer = number of filters

**Why Convolutions?**

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2020.png)

Through convolutions(filters) we have only 456 params but w/o it it is 3072 * 4704 = 15 million params

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2021.png)

sparse connection - means that the 1 pixel of the output is connected to only 9 of the pixel in the input (since filter = 3 * 3) 

Through these two mechanisms, a neural network has a lot fewer parameters which allows it to be trained with smaller training cells and is less prone to be overfitting.

- Convolutional structure helps the neural network encode the fact that an image shifted a few pixels should result in pretty similar features and should probably be assigned the same output label.
- And the fact that you are applying the same filter in all the positions of the image, both in the early layers and in the late layers that helps a neural network automatically learn to be more robust or to better capture the desirable property of translation invariance.

image → convolutional layers → fully connected layers → output layer

W, b ⇒ randomly initialised

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2022.png)

quiz :

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2023.png)

### SEQUENTIAL VS FUNCTIONAL API

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2024.png)

**documentation :**

seq model = [https://www.tensorflow.org/guide/keras/sequential_model](https://www.tensorflow.org/guide/keras/sequential_model)
func_ api = [https://www.tensorflow.org/guide/keras/functional_api](https://www.tensorflow.org/guide/keras/functional_api)

## WEEK 2 - classic neural networks

### Learning Objectives

---

- Implement the basic building blocks of ResNets in a deep neural network using Keras
- Train a state-of-the-art neural network for image classification
- Implement a skip connection in your network
- Create a dataset from a directory
- Preprocess and augment data using the Keras Sequential API
- Adapt a pretrained model to new data and train a classifier using the Functional API and MobileNet
- Fine-tine a classifier's final layers to improve accuracy

**Why look at case studies**

It is helpful in taking someone else's neural network architecture and applying that to another problem.

- Classic networks
    - LeNet-5
    - AlexNet
    - VGG
- ResNet
- Inception

LeNeT - 5

• Back then, people used sigmoid and tanh nonlinearities, not relu. Back in 1998 when the corresponding paper of LeNet - 5 was written padding wasn't used.

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2025.png)

finally output is the 1000 classes that the object could be

AlexNet

• AlexNet has a lot of similarities to LeNet (60,000 parameters), but it is much bigger (60 million parameters).(lot of hidden layers )

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2026.png)

uniformity, but large NN with lots of parameters ( 16 layers )

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2027.png)

- Filters are always `3x3` with a stride of `1` and are always `same` convolutions.
- VGG-16 has 16 layers that have weights. A total of about 138 million parameters. Pretty large even by modern standards.
- It is the simplicity, or the uniformity, of the VGG-16 architecture made it quite appealing.
    - There is a few conv-layers followed by a pooling layer which reduces the height and width by a factor of `2`.
    - Doubling through every stack of conv-layers is a simple principle used to design the architecture of this network.
- The main downside is that you have to train a large number of parameters.

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2028.png)

a^[l] skips 2 layers to pass deeper into the NN. These connections are called as skip connections

These residual blocks are combined to form residual network

As the number of layers gets deeper, the training error gets worse because its gets harder and harder for the optimization algorithm to train

But in resNets the training error keeps on going down 
When we make NN without skip connections, it gets very difficult to choose parameters

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2029.png)

its easy for the residual layers to learn the identity function

Why resnets work?

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2030.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2031.png)

suppose we had a big NN. Now we add a residual block into it. 

- If we use `L2` regularization on `a[l+2]=g(Z[l+2]+a[l])=g(W[l+2]a[l+1]+b[l+2]+a[l])`, and if the value of `W[l+2],b[l+2]` shrink to zero, then `a[l+2]=g(a[l])=a[l]` since we use `relu` activation and `a[l]` is also non-negative. So we just get back `a[l]`. This shows that the identity function is easy for residual block to learn.It's easy to get `a[l+2]` equals to `a[l]` because of this skip connection. What this means is that adding these two layers in the neural network doesn't really hurt the neural network's ability to do as well as this simpler network without these two extra layers, because it's quite easy for it to learn the identity function to just copy `a[l]` to `a[l+2]` despite the addition of these two layers.So adding two extra layers or adding this residual block to somewhere in the middle or the end of this big neural network doesn't hurt performance. It is easier to go from a decent baseline of not hurting performance and then gradient descent can only improve the solution from there.

*About dimensions*:

- In `a[l+2]=g(Z[l+2]+a[l])` we're assuming that `Z[l+2]` and `a[l]` have the same dimension. So what we see in ResNet is a lot of use of **same convolutions.**
- In case the input and output have different dimensions, we can add an extra matrix `W_s` so that `a[l+2] = g(Z[l+2] + W_s * a[l])`. The matrix `W_s` could be a matrix of parameters we learned or could be a fixed matrix that just implements zero paddings.

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2032.png)

To turn this into a ResNet, you add those extra skip connections and there are a lot of `3x3` convolutions and most of these are `3x3` same convolutions and that's why you're adding equal dimension feature vectors. There are occasionally pooling layers and in these cases you need to make an adjustment to the dimension by the matrix `W_s`.

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2033.png)

**Practice advices on ResNet**:

- Very deep "plain" networks don't work in practice because they are hard to train due to vanishing gradients.
- The skip-connections help to address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function.
- There are two main types of blocks: The identity block and the convolutional block.
- Very deep Residual Networks are built by stacking these blocks together.

MORE info : [https://chatgpt.com/share/05e4677d-ac0f-4a11-81c0-4892be5eea7b](https://chatgpt.com/share/05e4677d-ac0f-4a11-81c0-4892be5eea7b)

**Networks in Networks and 1x1 Convolutions**

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2034.png)

The 1×1 convolutional layer is equivalent to *the fully-connected layer*, when applied on a per pixel basis.

- You can take every pixel as an *example* with `n_c[l]` input values (channels) and the output layer has `n_c[l+1]` nodes. The kernel is just nothing but the weights.
- Thus the 1x1 convolutional layer requires `n_c[l+1] x n_c[l]` weights and the bias.

The 1x1 convolutional layer is actually doing something pretty non-trivial and adds non-linearity to your neural network and allow you to decrease or keep the same or if you want, increase the number of channels in your volumes.

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2035.png)

use of 1 * 1 convolutions

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2036.png)

Purpose and Benefits

- **Dimensionality Reduction or Expansion:**
    - 1x1 convolutions can reduce the depth of the input volume while maintaining the spatial dimensions. This is particularly useful for reducing the computational cost and number of parameters.
    - They can also expand the depth by generating more feature maps, allowing the network to learn more complex features.
- **Adding Non-linearity:**
    - After applying a 1x1 convolution, a non-linear activation function (e.g., ReLU) is typically applied.
    - This combination introduces non-linearity into the network, enabling it to learn more complex representations.
- **Feature Mixing:**
    - 1x1 convolutions mix the input channels and create new combinations of features, which can be important for capturing cross-channel correlations.

**Inception Network Motivation**

When designing a layer for a ConvNet, you might have to pick, do you want a 1 by 3 filter, or 3 by 3, or 5 by 5, or do you want a pooling layer? What the inception network does is it says, why shouldn't do them all? And this makes the network architecture more complicated, but it also works remarkably well.

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2037.png)

And the basic idea is that instead of you need to pick one of these filter sizes or pooling you want and commit to that, you can do them all and just concatenate all the outputs, and let the network learn whatever parameters it wants to use, whatever the combinations of these filter sizes it wants. Now it turns out that there is a problem with the inception layer as we've described it here, which is *computational cost*.

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2038.png)

high computational cost = 120 million

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2039.png)

The output size is 28 * 28 * 32 and each filter is of size 5 * 5 * 32 so, we need to do computations = 28 * 28 * 32 *  5 * 5 * 32 cuz each output pixel is formed by filter being applied on it

Using the idea of  1 * 1 convolutions, we can reduce the cost  by 1/ 10 th

so we first shrunk it to a much smaller intermediate volume ( also called as bottle neck )
So, we  use 1 * 1 convolutions (cost = filter size * output size  == amount of multiplications we need to do)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2040.png)

- Does reducing the size of the NN Affect performance?
⇒ As long as we are using inception network module along with the bottle neck, the performance doesn’t seem to hurt that much

Lets make this inception module :

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2041.png)

**combining many such  inception modules gives inception network**

some pooling layers are there in between to change the height and width

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2042.png)

the side branches use some hidden layer and try to use that to make a prediction

- **DEPTH WISE SEPERABLE CONVOLUTION (building block of mobile net as it reduces cost significantly)**

computation cost == total number of multiplications

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2043.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2044.png)

step 1 : first apply depth wise (figure shows how depthwise works)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2045.png)

\

step 2 : Then pointwise (figure shows how pointwise works)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2046.png)

so, the  computational cost decreases in depth wise seperable convolution

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/63a79b8f-5fd1-4a87-9d8e-cfedf5b07047.png)

roughly, the formula for how cheap the depthwise seperable convolution is :

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2047.png)

so, a depthwise seperable convolution is the building block of a convNet

Note that in the diagram below, $n_c$ = 6 but we have shown only 3 layers for diagram simplicity

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2048.png)

[**MobileNet Architecture**](https://www.notion.so/Course-4-CNNs-499d3029988940279d1fd7b9980901ce?pvs=21)

- **MOBILE NET (another NN architechture)**

(uses less expensive depthwise seperable convolution operator)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2049.png)

Mobile Net v2 architechture -

The last step involves shrinking of the volume which is also called as the **projection** step(because we re projecting down in volume)

In MobileNet v2, the bottleneck layers use a three-step process:

1. **Expansion**: A 1x1 convolution increases the number of channels.
2. **Depthwise Convolution**: A 3x3 depthwise convolution processes each channel separately.
3. **Projection**: Another 1x1 convolution reduces the number of channels back down.

WHY MOBILENETV1 IS BETTER THAN MOBILENETV2 ?

1) The **expansion** in the internal of the bottleneck block allows the NN to **learn richer and more complex functions.**

2) while deploying on a device like a mobile, we are often on heavy memory constraints.So, the bottle neck block uses the **pointwise convolution** to project it into a smaller set of values

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2050.png)

- **EFFICIENTNet**

How to tune mobilenet on the basis of devices(eg we are implementing a cv application for different brands of phone with different computational powers)

3 things we can do to scale things up :
1) use a higher resolution image(h)
2) make the network much deeper(w)
3) make the layers wider (d)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2051.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2052.png)

**Practical advices for using ConvNets**

**Using Open-Source Implementation**

- Starting with open-source implementations is a better way, or certainly a faster way to get started on a new project.
- One of the advantages of doing so also is that sometimes these networks take a long time to train, and someone else might have used multiple GPUs and a very large dataset to pretrain some of these networks. And that allows you to do transfer learning using these networks.

### Transfer Learning

1. **Leveraging Pre-trained Models**:
    - Training deep neural networks from scratch can be time-consuming and computationally expensive. It often requires powerful hardware and can take weeks or months.
    - Instead of starting from scratch, you can use pre-trained models. These models have been trained on large datasets (e.g., ImageNet) by other researchers and are available as open-source implementations along with their pre-trained weights.
2. **Adapting Pre-trained Models to New Tasks**:
    - When you have a small dataset for your specific image classification problem, you can download a pre-trained model and its weights.
    - Typically, these pre-trained models are designed to output a fixed set of labels (e.g., 1000 ImageNet classes). To adapt the model to your specific task, you can remove the final softmax layer (which outputs the pre-trained classes) and replace it with a new softmax layer that matches the number of classes in your dataset.
3.  ed up training, you can freeze the parameters (weights) of the early layers in the pre-trained model. Freezing means that during training, these layers' parameters will not be updated.
    - Most deep learning frameworks allow you to set layers as non-trainable by setting the `trainable` parameter to `False`.
4. **Pre-computing Activations**:
    - The early frozen layers act as fixed feature extractors. Since their parameters do not change, their output (activations) for a given input will always be the same.
    - To optimize training, you can pre-compute these activations and save them to disk. This way, you don't have to recompute them for each training epoch, which can save a lot of computation time.
5. **Adjusting the Number of Frozen Layers**:
    - The amount of data you have determines how many layers you should freeze:
        - **Small Dataset**: Freeze more layers. This leverages the feature extraction capability of the pre-trained model and reduces the risk of overfitting.
        - **Large Dataset**: Freeze fewer layers or even none at all. With more data, the model can learn task-specific features, and using the pre-trained weights as an initialization helps in faster convergence compared to random initialization.

**Data Augmentation**

Having more data will help all computer vision tasks.

*Some common data augmentation in computer vision*:

- Mirroring
- Random cropping
- Rotation
- Shearing
- Local warping

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2053.png)

*Color shifting*: Take different values of R, G and B and use them to *distort the color channels*. In practice, the values R, G and B are drawn from some probability distribution. This makes your learning algorithm more robust to changes in the colors of your images.

- One of the ways to implement color distortion uses an algorithm called PCA. The details of this are actually given in the AlexNet paper, and sometimes called PCA Color Augmentation.
    - If your image is mainly purple, if it mainly has red and blue tints, and very little green, then PCA Color Augmentation, will add and subtract a lot to red and blue, where it balance [inaudible] all the greens, so kind of keeps the overall color of the tint the same.

**A pretty common way of implementing data augmentation is to really have one thread, or more,  that is responsible for loading the data and implementing distortions, and then passing that to some other thread that then does the training.**

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2054.png)

- Often the data augmentation and training process can run in parallel.
- Similar to other parts of training a deep neural network, the data augmentation process also has a few hyperparameters, such as how much color shifting do you implement and what parameters you use for random cropping.

**State of Computer Vision**

- Image recognition: the problem of looking at a picture and telling you is this a cat or not.
- Object detection: look in the picture and actually you're putting the bounding boxes are telling you where in the picture the objects, such as the car as well. The cost of getting the bounding boxes is more expensive to label the objects.

*Data vs. hand-engineering*:

- Having a lot of data: simpler algorithms as well as less hand-engineering. So less needing to carefully design features for the problem, but instead you can have a giant neural network, even a simpler architecture.
- Don't have much data: more hand-engineering ("hacks")

*Two sources of knowledge*:

- Labeled data, (x,y)
- Hand-engineering: features / network architecture / other components

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2055.png)

**Tips for doing well on benchmarks/winning competitions**:

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2056.png)

![Untitled](Course%204%20CNNs%20499d3029988940279d1fd7b9980901ce/Untitled%2057.png)